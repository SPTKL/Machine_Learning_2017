{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook is mainly based on pgmpy's introductory notebooks.  You will need to install the pgmpy (\"Probabilistic Graphical Models for Python\") package if it is not already installed (I used: pip install pgmpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Models\n",
    "We can take the example of the student model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Image('./student_full_param.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pgmpy we define the network structure and the CPDs separately and then associate them with the structure. Here's an example for defining the above model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.factors.discrete import TabularCPD\n",
    "\n",
    "# Defining the model structure. We can define the network by just passing a list of edges.\n",
    "model = BayesianModel([('D', 'G'), ('I', 'G'), ('G', 'L'), ('I', 'S')])\n",
    "\n",
    "# Defining individual CPDs.\n",
    "cpd_d = TabularCPD(variable='D', variable_card=2, values=[[0.6, 0.4]])\n",
    "cpd_i = TabularCPD(variable='I', variable_card=2, values=[[0.7, 0.3]])\n",
    "\n",
    "# The representation of CPD in pgmpy is a bit different than the CPD shown in the above picture. In pgmpy the colums\n",
    "# are the evidences and rows are the states of the variable. So the grade CPD is represented like this:\n",
    "#\n",
    "#    +---------+---------+---------+---------+---------+\n",
    "#    | diff    | intel_0 | intel_0 | intel_1 | intel_1 |\n",
    "#    +---------+---------+---------+---------+---------+\n",
    "#    | intel   | diff_0  | diff_1  | diff_0  | diff_1  |\n",
    "#    +---------+---------+---------+---------+---------+\n",
    "#    | grade_0 | 0.3     | 0.05    | 0.9     | 0.5     |\n",
    "#    +---------+---------+---------+---------+---------+\n",
    "#    | grade_1 | 0.4     | 0.25    | 0.08    | 0.3     |\n",
    "#    +---------+---------+---------+---------+---------+\n",
    "#    | grade_2 | 0.3     | 0.7     | 0.02    | 0.2     |\n",
    "#    +---------+---------+---------+---------+---------+\n",
    "\n",
    "cpd_g = TabularCPD(variable='G', variable_card=3, \n",
    "                   values=[[0.3, 0.05, 0.9,  0.5],\n",
    "                           [0.4, 0.25, 0.08, 0.3],\n",
    "                           [0.3, 0.7,  0.02, 0.2]],\n",
    "                  evidence=['I', 'D'],\n",
    "                  evidence_card=[2, 2])\n",
    "\n",
    "cpd_l = TabularCPD(variable='L', variable_card=2, \n",
    "                   values=[[0.1, 0.4, 0.99],\n",
    "                           [0.9, 0.6, 0.01]],\n",
    "                   evidence=['G'],\n",
    "                   evidence_card=[3])\n",
    "\n",
    "cpd_s = TabularCPD(variable='S', variable_card=2,\n",
    "                   values=[[0.95, 0.2],\n",
    "                           [0.05, 0.8]],\n",
    "                   evidence=['I'],\n",
    "                   evidence_card=[2])\n",
    "\n",
    "# Associating the CPDs with the network\n",
    "model.add_cpds(cpd_d, cpd_i, cpd_g, cpd_l, cpd_s)\n",
    "\n",
    "# check_model checks for the network structure and CPDs and verifies that the CPDs are correctly \n",
    "# defined and sum to 1.\n",
    "model.check_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can now call some methods on the BayesianModel object.\n",
    "model.get_cpds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(model.get_cpds('G'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print model.predecessors('G')\n",
    "print model.successors('G')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independencies in Bayesian Networks\n",
    "\n",
    "Independence relationships implied by the network structure of a Bayesian Network can be categorized into 2 types:\n",
    "\n",
    "__Local Independencies:__ Any variable in the network is independent of its non-descendents given its parents. Mathematically it can be written as: $$ (X \\perp NonDesc(X) | Pa(X) $$\n",
    "where $ NonDesc(X) $ is the set of variables which are not descendents of $ X $ and $ Pa(X) $ is the set of variables which are parents of $ X $.\n",
    "\n",
    "\n",
    "__Global Independencies:__ For discussing global independencies in Bayesian Networks we need to look at the various network structures possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting the local independencies of a variable.\n",
    "print model.nodes()\n",
    "for thenode in model.nodes():\n",
    "    print model.local_independencies(thenode)\n",
    "# Or equivalently, you could write: model.local_independencies(model.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Active trail: For any two variables A and B in a network if any change in A influences the values of B then we say\n",
    "# that there is an active trail (or unblocked path) between A and B.\n",
    "# In pgmpy, active_trail_nodes gives a set of nodes which are affected by any change in the node passed in the argument.\n",
    "for thenode in model.nodes():\n",
    "    print model.active_trail_nodes(thenode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for thenode in model.nodes():\n",
    "    print model.active_trail_nodes(thenode, observed='G')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional Probability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pgmpy.inference import VariableElimination\n",
    "infer = VariableElimination(model)\n",
    "\n",
    "# Marginal distribution of G\n",
    "print(infer.query(['G'])['G'])\n",
    "\n",
    "# Conditional distribution of G given ~D, I\n",
    "print(infer.query(['G'], evidence={'D': 0, 'I': 1}) ['G'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Predicting values for new data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Most likely value (marginal)\n",
    "print infer.map_query('G')\n",
    "\n",
    "# Most likely value (conditional on ~D, I)\n",
    "print infer.map_query('G', evidence={'D': 0, 'I': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice #1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image('./Cloud.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Build the Bayesian model and add all the CPDs.  Print out all the local independencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2. Get P(W), P(W|C=f, S=t, R=t), P(W|C=f,S=f).  Predict the most likely value of W in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learning from Data\n",
    "\n",
    "This section will be about obtaining a Bayesian network, given a set of sample data. Learning a Bayesian network can be split into two problems:\n",
    "\n",
    " **Parameter learning:** Given a set of data records and a DAG that captures the dependencies between the variables, estimate the (conditional) probability distributions of the individual variables.\n",
    " \n",
    " **Structure learning:** Given a set of data records, estimate a DAG that captures the dependencies between the variables.\n",
    " \n",
    "We will illustrate how parameter learning and structure learning can be done with pgmpy. Currently, the library supports:\n",
    "\n",
    " - Parameter learning for *discrete* nodes:\n",
    "   - Maximum Likelihood Estimation\n",
    "   - Bayesian Estimation\n",
    "\n",
    "- Structure learning for *discrete*, *fully observed* networks:\n",
    "   - Score-based structure estimation (BIC/BDeu/K2 score; exhaustive search, hill climb/tabu search)\n",
    "   - Constraint-based structure estimation (PC)\n",
    "\n",
    "\n",
    "### (1) Parameter Learning\n",
    "\n",
    "Suppose we have the following data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.DataFrame(data={'fruit': [\"banana\", \"apple\", \"banana\", \"apple\", \"banana\",\"apple\", \"banana\", \n",
    "                                    \"apple\", \"apple\", \"apple\", \"banana\", \"banana\", \"apple\", \"banana\",], \n",
    "                          'tasty': [\"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \n",
    "                                    \"yes\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"no\"], \n",
    "                          'size': [\"large\", \"large\", \"large\", \"small\", \"large\", \"large\", \"large\",\n",
    "                                    \"small\", \"large\", \"large\", \"large\", \"large\", \"small\", \"small\"]})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the variables are related as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pgmpy.models import BayesianModel\n",
    "model = BayesianModel([('fruit', 'tasty'), ('size', 'tasty')])  # fruit -> tasty <- size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter learning is the task of estimating the values of the conditional probability distributions (CPDs), for the variables `fruit`, `size`, and `tasty`. \n",
    "\n",
    "#### State counts\n",
    "\n",
    "To make sense of the given data, we can start by counting how often each state of the variable occurs. If the variable is dependent on parents, the counts are done conditionally on the parents states, i.e., separately for each parent configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pgmpy.estimators import ParameterEstimator\n",
    "pe = ParameterEstimator(model, data)\n",
    "print(pe.state_counts('fruit'))  # unconditional\n",
    "print \"\\n\"\n",
    "print(pe.state_counts('tasty'))  # conditional on fruit and size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum Likelihood Estimation\n",
    "\n",
    "A natural estimate for the CPDs is to simply use the *relative frequencies* with which the variable states have occured. We observed `7 apples` among a total of `14 fruits`, so we might guess that about `50%` of `fruits` are `apples`.\n",
    "\n",
    "This approach is *Maximum Likelihood Estimation (MLE)*. According to MLE, we should fill the CPDs in such a way, that $P(\\text{data}\\:|\\:\\text{model})$ is maximal. This is achieved when using the *relative frequencies*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "mle = MaximumLikelihoodEstimator(model, data)\n",
    "print(mle.estimate_cpd('fruit'))  # unconditional\n",
    "print(mle.estimate_cpd('tasty'))  # conditional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While very straightforward, the ML estimator has the problem of *overfitting* to the data. In above CPD, the probability of a large banana being tasty is estimated at `0.833`, because `5` out of `6` observed large bananas were tasty. Fine. But note that the probability of a small banana being tasty is estimated at `0.0`, because we  observed only one small banana and it happened to be not tasty. But that should hardly make us certain that small bananas aren't tasty! We simply do not have enough observations to rely on the observed frequencies. If the observed data is not representative for the underlying distribution, ML estimations will be extremly far off. \n",
    "\n",
    "When estimating parameters for Bayesian networks, lack of data is a frequent problem. Even if the total sample size is very large, the fact that state counts are done conditionally for each parents configuration causes immense fragmentation. If a variable has 3 parents that can each take 10 states, then state counts will be done seperately for `10^3 = 1000` parents configurations. This makes MLE very fragile and unstable for learning Bayesian Network parameters. A way to mitigate MLE's overfitting is *Bayesian Parameter Estimation*.\n",
    "\n",
    "#### Bayesian Parameter Estimation\n",
    "\n",
    "The Bayesian Parameter Estimator starts with already existing prior CPDs, that express our beliefs about the variables *before* the data was observed. Those \"priors\" are then updated, using the state counts from the observed data.\n",
    "\n",
    "One can think of the priors as consisting of *pseudo state counts*, that are added to the actual counts before normalization.  Unless one wants to encode specific beliefs about the distributions of the variables, one commonly chooses uniform priors, i.e. assume that all states are equally likely.\n",
    "\n",
    "A very simple prior is the so-called *K2* prior, which simply adds `1` to the count of every single state.  A somewhat more sensible choice of prior is *BDeu* (Bayesian Dirichlet equivalent uniform prior). For BDeu we need to specify an *equivalent sample size* `N` and then the pseudo-counts are the equivalent of having observed `N` uniform samples of each variable (and each parent configuration). In pgmpy:\n",
    "\n",
    "##### Parameters\n",
    "----------\n",
    "prior_type: 'dirichlet', 'BDeu', or 'K2'\n",
    "\n",
    "string indicting which type of prior to use for the model parameters.\n",
    "            \n",
    "- If 'prior_type' is 'dirichlet', the following must be provided:\n",
    "\n",
    "'pseudo_counts' = dirichlet hyperparameters; a dict containing, for each variable, a list with a \"virtual\" count for each variable state, that is added to the state counts.(lexicographic ordering of states assumed)\n",
    "            \n",
    "- If 'prior_type' is 'BDeu', then an 'equivalent_sample_size' must be specified instead of 'pseudo_counts'. This is equivalent to 'prior_type=dirichlet' and using uniform 'pseudo_counts' of `equivalent_sample_size/(node_cardinality*np.pro(parents_cardinalities))` for each node.\n",
    "'equivalent_sample_size' can either be a numerical value or a dict that specifies the size for each variable seperately.\n",
    "\n",
    "- A prior_type of 'K2' is a shorthand for 'dirichlet' + setting every pseudo_count to 1, regardless of the cardinality of the variable.\n",
    "\n",
    "##### Returns\n",
    "-------\n",
    "parameters: list\n",
    "\n",
    "List of TabularCPDs, one for each variable of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pgmpy.estimators import BayesianEstimator\n",
    "est = BayesianEstimator(model, data)\n",
    "\n",
    "print(est.estimate_cpd('tasty', prior_type='BDeu', equivalent_sample_size=10))\n",
    "print\n",
    "print(est.estimate_cpd('tasty', prior_type='K2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "\n",
    "np.random.seed(100)\n",
    "raw_data = np.random.randint(low=0, high=2, size=(1000, 3))\n",
    "data = pd.DataFrame(raw_data, columns=['D', 'I', 'G'])\n",
    "data.D=data.I+data.G\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = BayesianModel([(\"I\",\"D\"),(\"G\",\"D\")])\n",
    "model.fit(data, estimator=BayesianEstimator, prior_type='K2')\n",
    "for cpd in model.get_cpds():\n",
    "    print(\"CPD of {variable}:\".format(variable=cpd.variable))\n",
    "    print(cpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print data.head()\n",
    "model.predict(data.loc[:4,[\"I\",\"G\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Structure Learning\n",
    "\n",
    "To learn model structure (a DAG) from a data set, there are two broad techniques:\n",
    "\n",
    " - score-based structure learning\n",
    " - constraint-based structure learning\n",
    "\n",
    "We briefly discuss these approaches and give examples.\n",
    "\n",
    "### Score-based Structure Learning\n",
    "\n",
    "This approach construes model selection as an optimization task. It has two building blocks:\n",
    "\n",
    "- A _scoring function_ $s_D\\colon M \\to \\mathbb R$ that maps models to a numerical score, based on how well they fit to a given data set $D$.\n",
    "- A _search strategy_ to traverse the search space of possible models $M$ and select a model with optimal score.\n",
    "\n",
    "\n",
    "#### Scoring functions\n",
    "\n",
    "Commonly used scores to measure the fit between model and data are _Bayesian Dirichlet scores_ such as *BDeu* or *K2* and the _Bayesian Information Criterion_ (BIC, also called MDL)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bic: https://en.wikipedia.org/wiki/Bayesian_information_criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pgmpy.estimators import BdeuScore, K2Score, BicScore\n",
    "from pgmpy.models import BayesianModel\n",
    "\n",
    "# create random data sample with 3 variables, where Z is dependent on X, Y:\n",
    "data = pd.DataFrame(np.random.randint(0, 4, size=(5000, 2)), columns=list('XY'))\n",
    "data['Z'] = data['X'] + data['Y']\n",
    "\n",
    "bdeu = BdeuScore(data, equivalent_sample_size=5)\n",
    "k2 = K2Score(data)\n",
    "bic = BicScore(data)\n",
    "\n",
    "model0 = BayesianModel([]) # all independent\n",
    "model0.add_nodes_from(['X','Y','Z'])\n",
    "model1 = BayesianModel([('X', 'Z'), ('Y', 'Z')])  # X -> Z <- Y\n",
    "model2 = BayesianModel([('X', 'Z'), ('X', 'Y')])  # Y <- X -> Z\n",
    "\n",
    "# In each case, the best model (model1) has the least negative score\n",
    "print \"BDEU scores\"\n",
    "print \"Model 0:\",bdeu.score(model0),\"\\nModel 1:\",bdeu.score(model1),\"\\nModel 2:\",bdeu.score(model2)\n",
    "print \"\\nK2 scores\"\n",
    "print \"Model 0:\",k2.score(model0),\"\\nModel 1:\",k2.score(model1),\"\\nModel 2:\",k2.score(model2)\n",
    "print \"\\nBIC scores\"\n",
    "print \"Model 0:\",bic.score(model0),\"\\nModel 1:\",bic.score(model1),\"\\nModel 2:\",bic.score(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These should add to the BDEU score of model 1\n",
    "print(bdeu.local_score('X', parents=[]))\n",
    "print(bdeu.local_score('Y', parents=[]))\n",
    "print(bdeu.local_score('Z', parents=['X', 'Y']))\n",
    "\n",
    "print\n",
    "\n",
    "# And these should add to the BDEU score of model 2\n",
    "print(bdeu.local_score('X', parents=[]))\n",
    "print(bdeu.local_score('Y', parents=['X']))\n",
    "print(bdeu.local_score('Z', parents=['X']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search strategies\n",
    "\n",
    "The search space of DAGs is super-exponential in the number of variables and the above scoring functions allow for local maxima. The first property makes exhaustive search intractable for all but very small networks, while the second property can prevent efficient local optimization algorithms from always finding the optimal structure. Thus, identifiying the ideal structure is often not tractable. Nevertheless, heuristic search strategies often yield good results.\n",
    "\n",
    "If only few nodes are involved (read: less than 5), `ExhaustiveSearch` can be used to compute the score for every DAG and returns the best-scoring one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pgmpy.estimators import ExhaustiveSearch\n",
    "\n",
    "# Note: exhaustive search will be terribly expensive for more than a few variables\n",
    "es = ExhaustiveSearch(data, scoring_method=bic)\n",
    "best_model = es.estimate()\n",
    "print(best_model.edges())\n",
    "\n",
    "print(\"\\nAll DAGs by score:\")\n",
    "for score, dag in reversed(es.all_scores()):\n",
    "    print(score, dag.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once more nodes are involved, one needs to switch to heuristic search. `HillClimbSearch` implements a greedy local search that starts from the DAG `start` (default: disconnected DAG) and proceeds by iteratively performing single-edge manipulations that maximally increase the score. The search terminates once a local maximum is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pgmpy.estimators import HillClimbSearch\n",
    "\n",
    "# create some random data with dependencies\n",
    "np.random.seed(30)\n",
    "data = pd.DataFrame(np.random.randint(0, 3, size=(2500, 8)), columns=list('ABCDEFGH'))\n",
    "data['A'] += data['B'] + data['C']\n",
    "data['H'] = data['G'] - data['A']\n",
    "\n",
    "hc = HillClimbSearch(data, scoring_method=BicScore(data))\n",
    "best_model = hc.estimate()\n",
    "print(best_model.edges())\n",
    "\n",
    "# Note: it doesn't always find the \"correct\" network.  To see this, try a different random seed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = pd.read_csv(\"NYC_taxi_sample.csv\")\n",
    "print data.head()\n",
    "train,test=train_test_split(data,random_state=999,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1. Learn the Bayes Net structure.  You can choose your favorite search method and scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2. Learn the model parameters assuming the structure you learned in Part 1.  Also print out all of the local independencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3. Report the IS and OS classification accuracy for \"Manhattan\" using model.predict()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Constraint-based Structure Learning\n",
    "\n",
    "A different approach to build a DAG from data, attempting to correctly capture the directionality of causal relationships, is this:\n",
    "\n",
    "1. Identify independencies in the data set using hypothesis tests \n",
    "2. Construct a DAG according to identified independencies\n",
    "\n",
    "#### (Conditional) Independence Tests\n",
    "\n",
    "Independencies in the data can be identified using $\\chi^2$ conditional independence tests. To this end, constraint-based estimators in pgmpy have a `test_conditional_independence(X, Y, Zs)`-method, that performs a hypothesis test on the data sample. It allows to check if `X` is independent from `Y` given a set of variables `Zs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.estimators import ConstraintBasedEstimator\n",
    "\n",
    "data = pd.DataFrame(np.random.randint(0, 3, size=(2500, 8)), columns=list('ABCDEFGH'))\n",
    "data['A'] += data['B'] + data['C']\n",
    "data['H'] = data['G'] - data['A']\n",
    "data['E'] *= data['F']\n",
    "\n",
    "est = ConstraintBasedEstimator(data)\n",
    "\n",
    "print(est.test_conditional_independence('B', 'C'))          # independent\n",
    "print(est.test_conditional_independence('B', 'H'))          # dependent\n",
    "print(est.test_conditional_independence('B', 'E'))          # independent\n",
    "print(est.test_conditional_independence('B', 'H', ['A']))   # independent\n",
    "print(est.test_conditional_independence('A', 'G'))          # independent\n",
    "print(est.test_conditional_independence('A', 'G',  ['H']))  # dependent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`test_conditional_independence()` returns a triple `(chi^2, p_value, sufficient_data)`, consisting of the computed $\\chi^2$ test statistic, the `p_value` of the test, and a flag that indicates if the sample size was sufficient. The `p_value` is the probability of observing the computed $\\chi^2$ statistic or a higher $\\chi^2$ value, given the null hypothesis that X and Y are independent given Zs. \n",
    "\n",
    "This can be used to make independence judgements, at a given level of significance:\n",
    "\n",
    "p-values less than the chosen significance level $\\alpha$ reject the null hypothesis and conclude that X and Y are conditionally dependent given Zs.  p-values greater than or equal to $\\alpha$ fail to reject the null hypothesis, from which we will conclude that X and Y are conditionally independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_independent(X, Y, Zs=[], significance_level=0.05):\n",
    "    return est.test_conditional_independence(X, Y, Zs)[1] >= significance_level\n",
    "print(is_independent('B', 'C'))\n",
    "print(is_independent('B', 'H'))\n",
    "print(is_independent('B', 'E'))\n",
    "print(is_independent('B', 'H', ['A']))\n",
    "print(is_independent('A', 'G'))\n",
    "print(is_independent('A', 'G', ['H']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DAG (pattern) construction\n",
    "\n",
    "#### PC Algorithm for causal direction\n",
    "\n",
    "With a method for independence testing at hand, we can construct a DAG from the data set in three steps:\n",
    "\n",
    "*1. Construct an undirected skeleton - `estimate_skeleton()`\n",
    "\n",
    "*2. Orient compelled edges to obtain partially directed acyclic graph (PDAG; I-equivalence class of DAGs) - `skeleton_to_pdag()`\n",
    "\n",
    "Steps 1 and 2 form the PC algorithm. PDAGs are `DirectedGraph`s, that may contain bidirectional edges, to indicate that the orientation for the edge is not determined.\n",
    "\n",
    "####  The following step orients any remaining edges, essentially at random, but in a way which is consistent with the edge directions produced by PC.  This is useful if we want to use the learned structure to perform inference, but we should not make any assumptions re. causality from the edges directed in the last step.\n",
    "\n",
    " *3. Extend DAG pattern to a DAG by orienting the remaining edges in some consistent way - `pdag_to_dag()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skel, separating_sets = est.estimate_skeleton(significance_level=0.01)\n",
    "print(\"Undirected edges: \", skel.edges())\n",
    "\n",
    "pdag = est.skeleton_to_pdag(skel, separating_sets)\n",
    "print(\"PDAG edges:       \", pdag.edges())\n",
    "\n",
    "model = est.pdag_to_dag(pdag)\n",
    "print(\"DAG edges:        \", model.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the last step randomly oriented the E<-->F edge.  The other edges were oriented correctly by the PC algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taxi data example\n",
    "\n",
    "data = pd.read_csv(\"NYC_taxi_sample.csv\")\n",
    "est = ConstraintBasedEstimator(data)\n",
    "skel, separating_sets = est.estimate_skeleton(significance_level=0.01)\n",
    "print(\"Undirected edges: \", skel.edges())\n",
    "\n",
    "pdag = est.skeleton_to_pdag(skel, separating_sets)\n",
    "print(\"PDAG edges:       \", pdag.edges())\n",
    "\n",
    "model = est.pdag_to_dag(pdag)\n",
    "print(\"DAG edges:        \", model.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the PC algorithm identified four edges (tip<-->pass and manhattan/speed/dist) but was not able to infer the causal direction of any of the four.  The third step just orients all four at random, avoiding directed cycles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
