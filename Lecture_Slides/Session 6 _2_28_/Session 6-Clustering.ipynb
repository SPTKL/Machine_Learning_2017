{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage  # for hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means clustering\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First let's create some random data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_features=2, centers=2,n_samples=20,random_state=9)\n",
    "\n",
    "# display\n",
    "plt.scatter(X[:,0],X[:,1],c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what running k-means on this data would look like, step by step:\n",
    "\n",
    "# arbitrary assignment of centers\n",
    "cent=np.asarray([[-6,-6],[-2,-1]])\n",
    "plt.figure(figsize=(12,3))\n",
    "\n",
    "# first of four panels just has the original data\n",
    "plt.subplot(1,4,1)\n",
    "plt.scatter(X[:,0],X[:,1],c=\"r\")\n",
    "\n",
    "# for each k-means step\n",
    "for i in range(3):\n",
    "    \n",
    "    # show on a separate subplot\n",
    "    plt.subplot(1,4,i+2)\n",
    "    \n",
    "    # compute euclidean distance to each cluster center for each point\n",
    "    distance_to_cluster_0 = np.sum((X-cent[0,:])**2,axis=1)**0.5\n",
    "    distance_to_cluster_1 = np.sum((X-cent[1,:])**2,axis=1)**0.5\n",
    "    \n",
    "    # compute cluster assignment for each point\n",
    "    y_t = (distance_to_cluster_0 > distance_to_cluster_1) \n",
    "    \n",
    "    # plot points and centers\n",
    "    plt.scatter(X[:,0],X[:,1],c=y_t,cmap=plt.cm.cool)\n",
    "    plt.scatter(cent[0,0],cent[0,1],c=\"b\",s=100)\n",
    "    plt.scatter(cent[1,0],cent[1,1],c=\"r\",s=100)\n",
    "\n",
    "    # recompute centers\n",
    "    cent=np.asarray([[np.sum(X[:,0]*(1-y_t))/(np.sum(1-y_t)),np.sum(X[:,1]*(1-y_t))/np.sum(1-y_t)],\n",
    "            [np.sum(X[:,0]*(y_t))/np.sum(y_t),np.sum(X[:,1]*(y_t))/np.sum(y_t)]])\n",
    "\n",
    "# show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use k-means package from sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(random_state=234,n_clusters=2)\n",
    "res=km.fit(X)\n",
    "\n",
    "print 'Cluster assignments:',res.labels_\n",
    "print '\\nCluster centers:\\n',res.cluster_centers_\n",
    "print '\\nSum of squared errors:',res.inertia_\n",
    "\n",
    "# plot points and cluster assigments\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.scatter(X[:,0],X[:,1],c=res.labels_,cmap=plt.cm.cool)\n",
    "plt.scatter(res.cluster_centers_[0,0],res.cluster_centers_[0,1],c=\"b\",s=100)\n",
    "plt.scatter(res.cluster_centers_[1,0],res.cluster_centers_[1,1],c=\"r\",s=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Models (EM)\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Gaussian mixture models (EM) package from sklearn\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy import linalg\n",
    "import matplotlib as mpl\n",
    "gmm = GaussianMixture(n_components=2,random_state=100)\n",
    "res = gmm.fit(X)\n",
    "\n",
    "print 'Probabilities of belonging to cluster 1:\\n',res.predict_proba(X)[:,1]\n",
    "print '\\nCluster centers:\\n',res.means_\n",
    "print '\\nCluster covariances:\\n',res.covariances_\n",
    "print '\\nLog-likelihood per sample:',res.score(X)\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "splot = plt.subplot(111)\n",
    "\n",
    "# plot ellipses using mean and covariance matrix for each cluster\n",
    "colors = ['blue','red']\n",
    "for i in range(2):\n",
    "    mean = res.means_[i]\n",
    "    var = res.covariances_[i]\n",
    "    v, w = linalg.eigh(var)\n",
    "    angle = np.arctan2(w[0][1], w[0][0])\n",
    "    angle = 180. * angle / np.pi  # convert to degrees\n",
    "    v = 2. * np.sqrt(2.) * np.sqrt(v)\n",
    "    ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=colors[i])\n",
    "    ell.set_clip_box(splot.bbox)\n",
    "    ell.set_alpha(.5)\n",
    "    splot.add_artist(ell)\n",
    "\n",
    "# plot points and cluster assigments\n",
    "plt.scatter(X[:,0],X[:,1],c=res.predict_proba(X)[:,1],cmap=plt.cm.cool)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice #1.  Using 311 complaints data set to cluster and visualize NYC census tract areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "data311=pd.read_csv(\"311DataForClustering.csv\")\n",
    "test=preprocessing.normalize(data311.iloc[:,1:])\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: example with random cluster assignments\n",
    "random_clusters = np.random.randint(4,size=len(data311.index))\n",
    "res=pd.concat([pd.Series(data311.geoid),pd.Series(random_clusters,name='cluster')],axis=1)\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference for this function: 311 Project (Lingjing Wang, Cheng Qian, Constantine Kontokosta, Stanislav Sobolevsky)\n",
    "\n",
    "https://arxiv.org/pdf/1611.06660.pdf\n",
    "\n",
    "pip install pyshp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualization(result):\n",
    "    import shapefile\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as patches\n",
    "    from matplotlib.patches import Polygon\n",
    "    from matplotlib.collections import PatchCollection\n",
    "    #   -- input --\n",
    "    Boston_geoid=list(data311.geoid.unique())\n",
    "    sf = shapefile.Reader(\"nyct2010_16b/nyct2010.shp\")\n",
    "    recs    = sf.records()\n",
    "    test=pd.DataFrame(recs)\n",
    "    test.loc[:,'county']=0\n",
    "    test.loc[test.iloc[:,2]==\"Staten Island\",\"county\"]=\"085\"\n",
    "    test.loc[test.iloc[:,2]==\"Manhattan\",\"county\"]=\"061\"\n",
    "    test.loc[test.iloc[:,2]==\"Brooklyn\",\"county\"]=\"047\"\n",
    "    test.loc[test.iloc[:,2]==\"Bronx\",\"county\"]=\"005\"\n",
    "    test.loc[test.iloc[:,2]==\"Queens\",\"county\"]=\"081\"\n",
    "    test.loc[:,\"geoid\"]=0\n",
    "    test.loc[:,\"geoid\"]=\"36\"+test.county+test.iloc[:,3].apply(str)\n",
    "\n",
    "    shapes  = sf.shapes()\n",
    "    Nshp    = len(shapes)\n",
    "    cns     = []\n",
    "    for nshp in xrange(Nshp):\n",
    "        cns.append(recs[nshp][1])\n",
    "    cns = np.array(cns)\n",
    "\n",
    "    cmap = plt.cm.Spectral(np.linspace(0,1,max(result.iloc[:,-1])+1))\n",
    "\n",
    "    fig=plt.figure(figsize = (10,10)) \n",
    "    fig.add_subplot(111)\n",
    "    ax = fig.gca() \n",
    "    for nshp in xrange(Nshp):\n",
    "        if int(test.iloc[nshp,-1]) in Boston_geoid:\n",
    "            k=result[result.geoid==int(test.iloc[nshp,-1])].iloc[0,-1]\n",
    "            c=cmap[k][0:3]  \n",
    "            ptchs   = []\n",
    "            pts     = np.array(shapes[nshp].points)\n",
    "            prt     = shapes[nshp].parts\n",
    "            par     = list(prt) + [pts.shape[0]]\n",
    "            for pij in xrange(len(prt)):\n",
    "                ptchs.append(Polygon(pts[par[pij]:par[pij+1]]))\n",
    "            ax.add_collection(PatchCollection(ptchs,facecolor=c,edgecolor='k', linewidths=.5))\n",
    "        ax.axis('scaled')\n",
    "    #ax.set_title(\"NYC Clustering based on 311 service request data\")\n",
    "\n",
    "    import matplotlib.patches as mpatches\n",
    "    clum_num=len(result.iloc[:,-1].unique())\n",
    "\n",
    "    handles=[]\n",
    "    for t in range(clum_num):\n",
    "        locals()[\"patch_{}\".format(t)] = mpatches.Patch(color=cmap[t][0:3] , label='Cluster'+str(t+1))\n",
    "        handles.append(locals()[\"patch_{}\".format(t)])\n",
    "    #plt.axis('off')\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([],[])\n",
    "    plt.legend(handles=handles,loc='upper left',prop={'size':8})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization(result=res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now try running k-means with between 2 and 5 clusters.  For each n_clusters, report the impurity and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html\n",
    "\n",
    "https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/\n",
    "\n",
    "(as an alternative, could use sklearn.cluster.AgglomerativeClustering, but not such great visualization.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# same randomly generated points as above\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_features=2, centers=2,n_samples=20,random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linkage:\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage\n",
    "\n",
    "An (n−1) by 4 matrix Z is returned. At the i-th iteration, clusters with indices Z[i, 0] and Z[i, 1] are combined to form cluster n+i. A cluster with an index less than nn corresponds to one of the nn original observations. The distance between clusters Z[i, 0] and Z[i, 1] is given by Z[i, 2]. The fourth value Z[i, 3] represents the number of original observations in the newly formed cluster.\n",
    "\n",
    "### Single Linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_single = linkage(X, 'single')\n",
    "print Z_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    Z_single,\n",
    "    leaf_rotation=90.,  # rotates the x axis labels\n",
    "    leaf_font_size=12.,  # font size for the x axis labels\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form Clusters:\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.fcluster.html#scipy.cluster.hierarchy.fcluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set number of clusters or threshold distance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into three clusters\n",
    "fcluster(Z_single, 3, criterion='maxclust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold distance = 2.1; break all links longer than the threshold distance.\n",
    "# Note that higher threshold distance corresponds to fewer clusters.\n",
    "fcluster(Z_single, t=2.1, criterion='distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A better dendrogram function (re-used from the blog): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fancy_dendrogram(*args, **kwargs): # This code is resued from the blog.\n",
    "    max_d = kwargs.pop('max_d', None)\n",
    "    if max_d and 'color_threshold' not in kwargs:\n",
    "        kwargs['color_threshold'] = max_d\n",
    "    annotate_above = kwargs.pop('annotate_above', 0)\n",
    "\n",
    "    ddata = dendrogram(*args, **kwargs)\n",
    "\n",
    "    if not kwargs.get('no_plot', False):\n",
    "        plt.title('Hierarchical Clustering Dendrogram (truncated)')\n",
    "        plt.xlabel('sample index or (cluster size)')\n",
    "        plt.ylabel('distance')\n",
    "        for i, d, c in zip(ddata['icoord'], ddata['dcoord'], ddata['color_list']):\n",
    "            x = 0.5 * sum(i[1:3])\n",
    "            y = d[1]\n",
    "            if y > annotate_above:\n",
    "                plt.plot(x, y, 'o', c=c)\n",
    "                plt.annotate(\"%.3g\" % y, (x, y), xytext=(0, -5),\n",
    "                             textcoords='offset points',\n",
    "                             va='top', ha='center')\n",
    "        if max_d:\n",
    "            plt.axhline(y=max_d, c='k')\n",
    "    return ddata\n",
    "\n",
    "\n",
    "# t is distance threshold to form number of clusters.  p is number of visualization slots (see example)\n",
    "def distance_thr(Z,t,p): \n",
    "    plt.figure(figsize=(10,6))\n",
    "    fancy_dendrogram(\n",
    "        Z,\n",
    "        truncate_mode='lastp',\n",
    "        p=p,\n",
    "        leaf_rotation=90.,\n",
    "        leaf_font_size=12.,\n",
    "        show_contracted=True,\n",
    "        annotate_above=10,\n",
    "        max_d=t,\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_thr(Z_single,t=2.1,p=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_thr(Z_single,t=2.1,p=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice #2:\n",
    "\n",
    "Try 'complete' linkage (and maybe other linkage metrics if you have time), and compare your results with the single linkage. Do these algorithms give you the same results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the number of clusters:\n",
    "\n",
    "There are numerous criteria for choosing the number of clusters.  See, for example:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_features=2, centers=3,n_samples=50,random_state=9999)\n",
    "plt.scatter(X[:,0],X[:,1],c=\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z = linkage(X, 'single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_thr(Z,t=2.5,p=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette Coefficient\n",
    "\n",
    "One example of a metric used to choose number of clusters:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient\n",
    "\n",
    "For each sample, compute (b-a)/max(a,b), where:\n",
    "\n",
    "a: The mean distance between a sample and all other points in the same class.\n",
    "\n",
    "b: The mean distance between a sample and all other points in the next nearest cluster.\n",
    "\n",
    "Then take the mean over all samples.\n",
    "\n",
    "Larger silhouette score (closer to 1) = better clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Choose a range of cluster numbers that you would like to evalulate:\n",
    "range_n_clusters = range(2,10)\n",
    "Z = linkage(X, 'single')\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    cluster_labels=fcluster(Z, n_clusters, criterion='maxclust')    \n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters = {},\".format(n_clusters)+\" the average silhouette_score is : {}\".format(silhouette_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an alternative, look for an elbow in the within-cluster SSE (k-means example):\n",
    "SSE = []\n",
    "for i in range(2,10):\n",
    "    km = KMeans(n_clusters = i)\n",
    "    res=km.fit(X)\n",
    "    SSE.append(res.inertia_)\n",
    "print SSE\n",
    "\n",
    "plt.gca()\n",
    "plt.plot(range(2,10),SSE)\n",
    "plt.xlabel(\"Clusters\")\n",
    "plt.ylabel(\"Sum of squared errors\")\n",
    "plt.title(\"SSE vs. number of clusters\")\n",
    "plt.xlim(1.5,9.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leader Clustering \n",
    "\n",
    "Leader clustering will not be covered in this lab, but if you're interested you can see:\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.leaders.html#scipy.cluster.hierarchy.leaders"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
