{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process Regression\n",
    "\n",
    "### Example 1: Simple case with no noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example of Gaussian Process Regression is borrowed from:\n",
    "#\n",
    "# Author: Vincent Dubourg <vincent.dubourg@gmail.com>\n",
    "#         Jake Vanderplas <vanderplas@astro.washington.edu>\n",
    "#         Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>s\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel,WhiteKernel\n",
    "np.random.seed(1)\n",
    "\n",
    "def f(x):\n",
    "    return x * np.sin(x)\n",
    "\n",
    "x_obs = np.array([1.,3.,5.,6.,7.,8.]).reshape(-1,1)\n",
    "y_obs = f(x_obs).ravel()\n",
    "\n",
    "# Mesh the input space for evaluations of the real function, the prediction and\n",
    "# its MSE\n",
    "x_mesh = np.linspace(0,10,1000).reshape(-1,1)\n",
    "y_mesh = f(x_mesh)\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.plot(x_mesh, f(x_mesh), 'r:', label=u'$f(x) = x\\,\\sin(x)$')\n",
    "plt.plot(x_obs, y_obs, 'r.', markersize=10, label=u'Observations')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RBF.html\n",
    "\n",
    "The RBF kernel is a stationary kernel. It is also known as the “squared exponential” kernel. It is parameterized by a length-scale parameter length_scale>0, which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs X (anisotropic variant of the kernel). The isotropic kernel is given by:\n",
    "\n",
    "$k(x_i, x_j) = e^{-\\frac{1}{2} D\\left(\\frac{x_i}{lengthScale}, \\frac{x_j}{lengthScale}\\right)^2}$, where $D$ is Euclidean distance.\n",
    "\n",
    "For the anisotropic kernel, each component $k$ of $x_i$ and $x_j$ is divided by $lengthScale$ before computing the squared distance in that dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Gaussian Process model with a radial basis function kernel.\n",
    "kernel = RBF(1., (0.1, 10.)) # parameters of RBF kernel: length_scale, (length_scale_range_min, length_scale_range_max)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, random_state=0)\n",
    "\n",
    "# Fit to data using maximum likelihood estimation of the parameters\n",
    "gp.fit(x_obs, y_obs)\n",
    "\n",
    "# Make the prediction on the meshed x-axis (mean and standard deviation)\n",
    "y_mesh_pred, sigma = gp.predict(x_mesh, return_std=True)\n",
    "\n",
    "# Plot the function, the prediction and the 95% confidence interval\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.plot(x_mesh, y_mesh, 'r:', label=u'$f(x) = x\\,\\sin(x)$')\n",
    "plt.plot(x_obs, y_obs, 'r.', markersize=10, label=u'Observations')\n",
    "plt.plot(x_mesh, y_mesh_pred, 'b-', label=u'Prediction')\n",
    "plt.fill(np.concatenate([x_mesh, x_mesh[::-1]]),\n",
    "         np.concatenate([y_mesh_pred - 1.9600 * sigma,\n",
    "                        (y_mesh_pred + 1.9600 * sigma)[::-1]]),\n",
    "         alpha=.5, fc='b', ec='None', label='95% confidence interval')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.ylim(-10, 20)\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "print gp.log_marginal_likelihood_value_\n",
    "print gp.kernel_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens if the kernel bandwidth is too small or too large?\n",
    "# Try RBF(0.1,(0.1,0.1)) and RBF(10.,(10.,10.))\n",
    "# This is one reason why we like to specify a range of bandwidths and let RBF pick a good one.\n",
    "\n",
    "# Instantiate a Gaussian Process model with a radial basis function kernel.\n",
    "kernel = RBF(0.1, (0.1, 0.1)) # parameters of RBF kernel: length_scale, (length_scale_range_min, length_scale_range_max)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, random_state=0)\n",
    "\n",
    "# Fit to data using maximum likelihood estimation of the parameters\n",
    "gp.fit(x_obs, y_obs)\n",
    "\n",
    "# Make the prediction on the meshed x-axis (mean and standard deviation)\n",
    "y_mesh_pred, sigma = gp.predict(x_mesh, return_std=True)\n",
    "\n",
    "# Plot the function, the prediction and the 95% confidence interval\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.plot(x_mesh, y_mesh, 'r:', label=u'$f(x) = x\\,\\sin(x)$')\n",
    "plt.plot(x_obs, y_obs, 'r.', markersize=10, label=u'Observations')\n",
    "plt.plot(x_mesh, y_mesh_pred, 'b-', label=u'Prediction')\n",
    "plt.fill(np.concatenate([x_mesh, x_mesh[::-1]]),\n",
    "         np.concatenate([y_mesh_pred - 1.9600 * sigma,\n",
    "                        (y_mesh_pred + 1.9600 * sigma)[::-1]]),\n",
    "         alpha=.5, fc='b', ec='None', label='95% confidence interval')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.ylim(-10, 20)\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "print gp.log_marginal_likelihood_value_\n",
    "print gp.kernel_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2. Let's try a more difficult example with added noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "x_obs = np.linspace(0.1, 9.9, 100).reshape(-1,1)\n",
    "y_obs = f(x_obs).ravel()\n",
    "\n",
    "# add noise\n",
    "dy = 0.5 + 1.0 * np.random.random(y_obs.shape)\n",
    "noise = np.random.normal(0, dy)\n",
    "y_obs += noise\n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.plot(x_mesh, y_mesh, 'r:', label=u'$f(x) = x\\,\\sin(x)$')\n",
    "plt.plot(x_obs, y_obs, 'r.', markersize=10, label=u'Observations')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Using RBF without adding white noise kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Gaussian Process model.\n",
    "\n",
    "# Note: specifying 1.*RBF(...) instead of RBF() will allow it to fit a constant multiplicative factor as well.\n",
    "# Without it, performance is even worse in this example!\n",
    "kernel = 1.*RBF(1., (0.1, 10.))\n",
    "gp = GaussianProcessRegressor(kernel=kernel,random_state=1)\n",
    "\n",
    "# Fit to data using Maximum Likelihood Estimation of the parameters\n",
    "gp.fit(x_obs, y_obs)\n",
    "\n",
    "y_mesh_pred, sigma = gp.predict(x_mesh, return_std=True)\n",
    "\n",
    "# Plot the function, the prediction and the 95% confidence interval\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.plot(x_mesh, y_mesh, 'r:', label=u'$f(x) = x\\,\\sin(x)$')\n",
    "plt.errorbar(x_obs.ravel(), y_obs, dy, fmt='r.', markersize=10, label=u'Observations')\n",
    "plt.plot(x_mesh, y_mesh_pred, 'b-', label=u'Prediction')\n",
    "plt.fill(np.concatenate([x_mesh, x_mesh[::-1]]),\n",
    "         np.concatenate([y_mesh_pred - 1.9600 * sigma,\n",
    "                        (y_mesh_pred + 1.9600 * sigma)[::-1]]),\n",
    "         alpha=.5, fc='b', ec='None', label='95% confidence interval')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.ylim(-10, 20)\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.log_marginal_likelihood_value_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.kernel_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Adding whiteKernel to model noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 1.* RBF(length_scale=1., length_scale_bounds=(1e-3, 1e3)) \\\n",
    "    + WhiteKernel(noise_level=1., noise_level_bounds=(1e-10, 1e10))\n",
    "\n",
    "gp = GaussianProcessRegressor(kernel=kernel,random_state=1)\n",
    "\n",
    "# Fit to data using Maximum Likelihood Estimation of the parameters\n",
    "gp.fit(x_obs, y_obs)\n",
    "\n",
    "y_mesh_pred, sigma = gp.predict(x_mesh, return_std=True)\n",
    "\n",
    "# Plot the function, the prediction and the 95% confidence interval\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.plot(x_mesh, y_mesh, 'r:', label=u'$f(x) = x\\,\\sin(x)$')\n",
    "plt.errorbar(x_obs.ravel(), y_obs, dy, fmt='r.', markersize=10, label=u'Observations')\n",
    "plt.plot(x_mesh, y_mesh_pred, 'b-', label=u'Prediction')\n",
    "plt.fill(np.concatenate([x_mesh, x_mesh[::-1]]),\n",
    "         np.concatenate([y_mesh_pred - 1.9600 * sigma,\n",
    "                        (y_mesh_pred + 1.9600 * sigma)[::-1]]),\n",
    "         alpha=.5, fc='b', ec='None', label='95% confidence interval')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.ylim(-10, 20)\n",
    "plt.legend(loc='upper left')\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print gp.log_marginal_likelihood_value_\n",
    "print gp.kernel_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### c) Prediction\n",
    "\n",
    "We observe that the uncertainty in our predictions increases as we move away from the observed data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mesh_2 = np.linspace(x_mesh.min(), x_mesh.max() + 5, 1000)[:, np.newaxis]\n",
    "y_mesh_2_pred, y_mesh_2_std = gp.predict(x_mesh_2, return_std=True)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.scatter(x_obs, y_obs, c='k')\n",
    "plt.plot(x_mesh_2, f(x_mesh_2), 'r:', label=u'$f(x) = x\\,\\sin(x)$')\n",
    "plt.plot(x_mesh_2, y_mesh_2_pred)\n",
    "plt.fill_between(x_mesh_2[:, 0], y_mesh_2_pred - 1.96*y_mesh_2_std, y_mesh_2_pred + 1.96*y_mesh_2_std,\n",
    "                 alpha=0.5, color='k')\n",
    "plt.xlim(x_mesh_2.min(), x_mesh_2.max())\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice #1: Long-range time series prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "dy=np.random.randn(150)\n",
    "y=5*np.asarray(range(150))+dy*30\n",
    "x=np.asarray(range(150))\n",
    "plt.figure(figsize=(20,3))\n",
    "plt.plot(y,\"b:\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train=x[:100]\n",
    "x_test=x[100:]\n",
    "y_train=y[:100]\n",
    "y_test=y[100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to model y using x.\n",
    "\n",
    "#### Part 1. Try using an RBF kernel without whiteKernel. What are the parameters learned from the training data? Plot the predictions for both training and testing data together using the trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: Repeat this process using RBF kernel + whiteKernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Example 3: Modeling data with cyclical trends.  The data represents the number of international airline passengers, in thousands, by month from January 1949 through December 1960.  We are going to make long-range predictions for the last 64 months of data using the first 80 months for training.\n",
    "\n",
    "To do so, we are going to engineer a kernel which should capture all the different trends in the data, including:\n",
    "\n",
    "1) Long term, smooth rising trend: model by an RBF kernel with a long length scale\n",
    "\n",
    "2) Seasonal component: model by the product of an RBF kernel (with a long length scale) and a periodic kernel (ExpSineSquared), with periodicity fixed to a 12-month cycle.\n",
    "\n",
    "3) Medium term irregularities: model by a RationalQuadratic kernel\n",
    "\n",
    "4) Noise terms: model by a whiteKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel,WhiteKernel,RationalQuadratic,Exponentiation,ExpSineSquared\n",
    "dataset = pd.read_csv(\"GP_Inter_P.csv\")\n",
    "\n",
    "train=dataset[:80]\n",
    "test=dataset[80:]\n",
    "\n",
    "# Kernel with optimized parameters\n",
    "k1 = 10.**2 * RBF(length_scale=50.0)  # long term smooth rising trend\n",
    "k2 = 50.**2* RBF(length_scale=100.0) \\\n",
    "    * ExpSineSquared(length_scale=1.0, periodicity=12.0,\n",
    "                     periodicity_bounds=\"fixed\")  # seasonal component\n",
    "# medium term irregularities\n",
    "k3 = 10.**2 * RationalQuadratic(length_scale=1.0, alpha=1.0)\n",
    "k4 = 1.**2 * RBF(length_scale=0.1) + WhiteKernel(noise_level=0.1**2,\n",
    "                  noise_level_bounds=(1e-3, 1e3))  # noise terms\n",
    "    \n",
    "kernel = k1 + k2 + k3 + k4\n",
    "\n",
    "gp = GaussianProcessRegressor(kernel=kernel,random_state=1)\n",
    "\n",
    "# Fit to data using Maximum Likelihood Estimation of the parameters\n",
    "gp.fit(np.asarray(train.index).reshape(-1,1), np.asarray(train))\n",
    "y_pred, sigma = gp.predict(np.asarray(dataset.index).reshape(-1,1), return_std=True)\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.plot(range(len(dataset)), np.asarray(dataset), 'r.', markersize=10, label=u'Observations')\n",
    "plt.plot(range(len(dataset)), y_pred, 'b-', label=u'Prediction')\n",
    "plt.fill(np.concatenate([range(len(dataset)), range(len(dataset))[::-1]]),\n",
    "         np.concatenate([y_pred[:,0] - 1.9600 * sigma,\n",
    "                        (y_pred[:,0] + 1.9600 * sigma)[::-1]]),\n",
    "          alpha=.5, fc='b', ec='None', label='95% confidence interval')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.show()\n",
    "\n",
    "print gp.log_marginal_likelihood_value_\n",
    "print gp.kernel_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice 2: \n",
    "\n",
    "The data consists of the monthly average atmospheric CO2 concentrations (in parts per million by volume (ppmv)) collected at the Mauna Loa Observatory in Hawaii, from 1958-2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Data2=pd.read_csv(\"GP_CO2.csv\")\n",
    "print Data2.head()\n",
    "plt.plot(Data2.average)\n",
    "plt.show()\n",
    "train=Data2[:600]\n",
    "test=Data2[600:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given this data, your goal is to model  the monthly average atmospheric CO2 concentrations using the training data (first 600 months, through 2008) with a GP, then predict for the following years.  Report your kernel and the log-marginal likelihood of the data; let's see who can achieve the best fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process Classification (Quick Example on Iris Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "y = np.array(iris.target, dtype=int)\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "kernel = 1.0 * RBF([1.0])\n",
    "gpc_rbf = GaussianProcessClassifier(kernel=kernel).fit(X, y)\n",
    "\n",
    "# create a mesh to plot in\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "    # Plot the predicted probabilities. For that, we will assign a color to\n",
    "    # each point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "plt.subplot(1, 1, 1)\n",
    "Z = gpc_rbf.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape((xx.shape[0], xx.shape[1], 3))\n",
    "plt.imshow(Z, extent=(x_min, x_max, y_min, y_max), origin=\"lower\")\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=np.array([\"r\", \"g\", \"b\"])[y])\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.title(\"%s, LML: %.3f\" %\n",
    "          (\"RBF\", gpc_rbf.log_marginal_likelihood(gpc_rbf.kernel_.theta)))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
